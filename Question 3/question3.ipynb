{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjYBMFxc51rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6faaef8f-b1d7-419d-d6fc-139b4ac96b95"
      },
      "source": [
        "# Connect colab with drive to get dataset\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0oFl4gOR6ty"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf # version=2.3.0 tensorflow is used to build the MLP\r\n",
        "import pandas as pd # pandas is used to read/write .txt\r\n",
        "import matplotlib.pyplot as plt #used to display\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2WcvrUPSgfq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "c6614a78-2905-45b4-d7c1-84aa92338534"
      },
      "source": [
        "# Read data and divide train dataset for testing\r\n",
        "train = pd.read_table('/content/drive/MyDrive/Question 3/train_data.txt',delim_whitespace=True)\r\n",
        "train_data = train.iloc[0:9000,:]\r\n",
        "test_data = train.iloc[9000:10000,:]\r\n",
        "\r\n",
        "truth = pd.read_table('/content/drive/MyDrive/Question 3/train_truth.txt',delim_whitespace=True)\r\n",
        "train_truth = truth.iloc[0:9000,:]\r\n",
        "test_truth = truth.iloc[9000:10000,:]\r\n",
        "\r\n",
        "target_test_data = pd.read_table('/content/drive/MyDrive/Question 3/test_data.txt',delim_whitespace=True)\r\n",
        "target_test_data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.691383</td>\n",
              "      <td>0.640371</td>\n",
              "      <td>0.395866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.077463</td>\n",
              "      <td>0.981754</td>\n",
              "      <td>0.481786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.841863</td>\n",
              "      <td>0.986477</td>\n",
              "      <td>0.201718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.798518</td>\n",
              "      <td>0.497791</td>\n",
              "      <td>0.560932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.101176</td>\n",
              "      <td>0.199818</td>\n",
              "      <td>0.117547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>0.297295</td>\n",
              "      <td>0.166326</td>\n",
              "      <td>0.578395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>0.956991</td>\n",
              "      <td>0.255906</td>\n",
              "      <td>0.526024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>0.093013</td>\n",
              "      <td>0.103068</td>\n",
              "      <td>0.344130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>0.375849</td>\n",
              "      <td>0.734481</td>\n",
              "      <td>0.145424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>0.763681</td>\n",
              "      <td>0.169072</td>\n",
              "      <td>0.704779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            x1        x2        x3\n",
              "0     0.691383  0.640371  0.395866\n",
              "1     0.077463  0.981754  0.481786\n",
              "2     0.841863  0.986477  0.201718\n",
              "3     0.798518  0.497791  0.560932\n",
              "4     0.101176  0.199818  0.117547\n",
              "...        ...       ...       ...\n",
              "2495  0.297295  0.166326  0.578395\n",
              "2496  0.956991  0.255906  0.526024\n",
              "2497  0.093013  0.103068  0.344130\n",
              "2498  0.375849  0.734481  0.145424\n",
              "2499  0.763681  0.169072  0.704779\n",
              "\n",
              "[2500 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPrE_T33W4zx"
      },
      "source": [
        " \"\"\"\r\n",
        "Initialize the MLP model and set the layers\r\n",
        "I tried L2 regularization and dropout method to avoid overfitting,\r\n",
        "but it seemed to be useless and increased the MSE\r\n",
        "so I did not use them in the end\r\n",
        "\"\"\"\r\n",
        "model = tf.keras.Sequential(\r\n",
        "    [\r\n",
        "     tf.keras.layers.Dense(4,input_shape=(3,),activation='relu'),#,kernel_regularizer=tf.keras.regularizers.l2(0.001)\r\n",
        "     \r\n",
        "    #  tf.keras.layers.Dropout(0.01),\r\n",
        "     tf.keras.layers.Dense(4,activation='relu'),\r\n",
        "    #  tf.keras.layers.Dropout(0.01),\r\n",
        "     tf.keras.layers.Dense(1)\r\n",
        "    ]\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_G2irlHX-Cf",
        "outputId": "fce50b7a-4429-482c-9dee-d677431e56fd"
      },
      "source": [
        "# Check if the structure is right\r\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 16        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 41\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8VIMukhYlxw"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0045),\r\n",
        "      loss='mse',\r\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t59bmDPVgdL"
      },
      "source": [
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\r\n",
        "                            min_delta=0.0000,patience=10\r\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BoyGyPCaLBn",
        "outputId": "b66a3ea7-0076-47b3-c5f8-b8f1ea697e68"
      },
      "source": [
        "history = model.fit(test_data,test_truth,\r\n",
        "          epochs=500,\r\n",
        "          callbacks=[earlystop_callback],\r\n",
        "          validation_data=(test_data,test_truth)\r\n",
        "          )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "32/32 [==============================] - 1s 15ms/step - loss: 1.9334 - val_loss: 0.2196\n",
            "Epoch 2/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 0.1531\n",
            "Epoch 3/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 0.1111\n",
            "Epoch 4/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.0806\n",
            "Epoch 5/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.0545\n",
            "Epoch 6/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.0348\n",
            "Epoch 7/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0236\n",
            "Epoch 8/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0192\n",
            "Epoch 9/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0167\n",
            "Epoch 10/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.0149\n",
            "Epoch 11/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.0134\n",
            "Epoch 12/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0121\n",
            "Epoch 13/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0109\n",
            "Epoch 14/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0106 - val_loss: 0.0098\n",
            "Epoch 15/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0104 - val_loss: 0.0088\n",
            "Epoch 16/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0087 - val_loss: 0.0079\n",
            "Epoch 17/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0071\n",
            "Epoch 18/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0063\n",
            "Epoch 19/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0055\n",
            "Epoch 20/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 21/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 22/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0036\n",
            "Epoch 23/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 24/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 25/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 26/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 27/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 28/500\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 29/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 30/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 9.0690e-04\n",
            "Epoch 31/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.3438e-04 - val_loss: 7.8530e-04\n",
            "Epoch 32/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.1929e-04 - val_loss: 6.9076e-04\n",
            "Epoch 33/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.5306e-04 - val_loss: 6.1131e-04\n",
            "Epoch 34/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.2461e-04 - val_loss: 5.6555e-04\n",
            "Epoch 35/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.4774e-04 - val_loss: 5.0820e-04\n",
            "Epoch 36/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7841e-04 - val_loss: 4.7982e-04\n",
            "Epoch 37/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.5340e-04 - val_loss: 4.5570e-04\n",
            "Epoch 38/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7164e-04 - val_loss: 4.3652e-04\n",
            "Epoch 39/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.4581e-04 - val_loss: 4.2562e-04\n",
            "Epoch 40/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0604e-04 - val_loss: 4.1164e-04\n",
            "Epoch 41/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2957e-04 - val_loss: 4.0341e-04\n",
            "Epoch 42/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.9364e-04 - val_loss: 4.0178e-04\n",
            "Epoch 43/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8427e-04 - val_loss: 3.9219e-04\n",
            "Epoch 44/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.8853e-04 - val_loss: 3.9198e-04\n",
            "Epoch 45/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6129e-04 - val_loss: 3.8482e-04\n",
            "Epoch 46/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4949e-04 - val_loss: 3.8256e-04\n",
            "Epoch 47/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4951e-04 - val_loss: 3.7953e-04\n",
            "Epoch 48/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0010e-04 - val_loss: 3.7865e-04\n",
            "Epoch 49/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0084e-04 - val_loss: 3.7604e-04\n",
            "Epoch 50/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4047e-04 - val_loss: 3.7833e-04\n",
            "Epoch 51/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5949e-04 - val_loss: 3.7457e-04\n",
            "Epoch 52/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.0188e-04 - val_loss: 3.7253e-04\n",
            "Epoch 53/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6045e-04 - val_loss: 3.7122e-04\n",
            "Epoch 54/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.3909e-04 - val_loss: 3.8286e-04\n",
            "Epoch 55/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5551e-04 - val_loss: 3.7909e-04\n",
            "Epoch 56/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7139e-04 - val_loss: 3.6821e-04\n",
            "Epoch 57/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7036e-04 - val_loss: 3.6874e-04\n",
            "Epoch 58/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6662e-04 - val_loss: 3.6912e-04\n",
            "Epoch 59/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4759e-04 - val_loss: 3.6658e-04\n",
            "Epoch 60/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4248e-04 - val_loss: 3.7050e-04\n",
            "Epoch 61/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6388e-04 - val_loss: 3.6208e-04\n",
            "Epoch 62/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4366e-04 - val_loss: 3.6422e-04\n",
            "Epoch 63/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5934e-04 - val_loss: 3.6451e-04\n",
            "Epoch 64/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5463e-04 - val_loss: 3.5879e-04\n",
            "Epoch 65/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4591e-04 - val_loss: 3.5785e-04\n",
            "Epoch 66/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4452e-04 - val_loss: 3.6132e-04\n",
            "Epoch 67/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5469e-04 - val_loss: 3.5586e-04\n",
            "Epoch 68/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4151e-04 - val_loss: 3.5503e-04\n",
            "Epoch 69/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6674e-04 - val_loss: 3.5415e-04\n",
            "Epoch 70/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7762e-04 - val_loss: 3.5294e-04\n",
            "Epoch 71/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.9160e-04 - val_loss: 3.5645e-04\n",
            "Epoch 72/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2156e-04 - val_loss: 3.5093e-04\n",
            "Epoch 73/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4323e-04 - val_loss: 3.5771e-04\n",
            "Epoch 74/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7068e-04 - val_loss: 3.6095e-04\n",
            "Epoch 75/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5648e-04 - val_loss: 3.4940e-04\n",
            "Epoch 76/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6574e-04 - val_loss: 3.4539e-04\n",
            "Epoch 77/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6396e-04 - val_loss: 3.4465e-04\n",
            "Epoch 78/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.5865e-04 - val_loss: 3.4327e-04\n",
            "Epoch 79/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2608e-04 - val_loss: 3.4185e-04\n",
            "Epoch 80/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2999e-04 - val_loss: 3.4101e-04\n",
            "Epoch 81/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7568e-04 - val_loss: 3.3941e-04\n",
            "Epoch 82/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6132e-04 - val_loss: 3.3702e-04\n",
            "Epoch 83/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2948e-04 - val_loss: 3.3509e-04\n",
            "Epoch 84/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4096e-04 - val_loss: 3.3277e-04\n",
            "Epoch 85/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2155e-04 - val_loss: 3.3181e-04\n",
            "Epoch 86/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.3510e-04 - val_loss: 3.2951e-04\n",
            "Epoch 87/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4323e-04 - val_loss: 3.3304e-04\n",
            "Epoch 88/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2051e-04 - val_loss: 3.2580e-04\n",
            "Epoch 89/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2787e-04 - val_loss: 3.2655e-04\n",
            "Epoch 90/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2791e-04 - val_loss: 3.2251e-04\n",
            "Epoch 91/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1637e-04 - val_loss: 3.2062e-04\n",
            "Epoch 92/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2638e-04 - val_loss: 3.1894e-04\n",
            "Epoch 93/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2199e-04 - val_loss: 3.1755e-04\n",
            "Epoch 94/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8472e-04 - val_loss: 3.1527e-04\n",
            "Epoch 95/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1962e-04 - val_loss: 3.1556e-04\n",
            "Epoch 96/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9918e-04 - val_loss: 3.1194e-04\n",
            "Epoch 97/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0059e-04 - val_loss: 3.1110e-04\n",
            "Epoch 98/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2115e-04 - val_loss: 3.1301e-04\n",
            "Epoch 99/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1528e-04 - val_loss: 3.0791e-04\n",
            "Epoch 100/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1766e-04 - val_loss: 3.0679e-04\n",
            "Epoch 101/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1675e-04 - val_loss: 3.0581e-04\n",
            "Epoch 102/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.3220e-04 - val_loss: 3.0554e-04\n",
            "Epoch 103/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9976e-04 - val_loss: 3.0485e-04\n",
            "Epoch 104/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1475e-04 - val_loss: 3.0928e-04\n",
            "Epoch 105/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2317e-04 - val_loss: 3.0074e-04\n",
            "Epoch 106/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1966e-04 - val_loss: 2.9903e-04\n",
            "Epoch 107/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2069e-04 - val_loss: 3.0634e-04\n",
            "Epoch 108/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0915e-04 - val_loss: 2.9843e-04\n",
            "Epoch 109/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0006e-04 - val_loss: 2.9565e-04\n",
            "Epoch 110/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9220e-04 - val_loss: 3.0613e-04\n",
            "Epoch 111/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9357e-04 - val_loss: 2.9686e-04\n",
            "Epoch 112/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8966e-04 - val_loss: 2.9584e-04\n",
            "Epoch 113/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0424e-04 - val_loss: 2.9283e-04\n",
            "Epoch 114/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8761e-04 - val_loss: 3.0188e-04\n",
            "Epoch 115/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7928e-04 - val_loss: 2.9639e-04\n",
            "Epoch 116/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9010e-04 - val_loss: 2.8710e-04\n",
            "Epoch 117/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9261e-04 - val_loss: 2.8613e-04\n",
            "Epoch 118/500\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.9050e-04 - val_loss: 2.8474e-04\n",
            "Epoch 119/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9677e-04 - val_loss: 2.8260e-04\n",
            "Epoch 120/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9083e-04 - val_loss: 2.8840e-04\n",
            "Epoch 121/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9650e-04 - val_loss: 2.8190e-04\n",
            "Epoch 122/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8241e-04 - val_loss: 2.8191e-04\n",
            "Epoch 123/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6232e-04 - val_loss: 2.7860e-04\n",
            "Epoch 124/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8500e-04 - val_loss: 2.7945e-04\n",
            "Epoch 125/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6250e-04 - val_loss: 2.7612e-04\n",
            "Epoch 126/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9344e-04 - val_loss: 2.7459e-04\n",
            "Epoch 127/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7449e-04 - val_loss: 2.8093e-04\n",
            "Epoch 128/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8759e-04 - val_loss: 2.9064e-04\n",
            "Epoch 129/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6107e-04 - val_loss: 2.7246e-04\n",
            "Epoch 130/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7999e-04 - val_loss: 2.7273e-04\n",
            "Epoch 131/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7407e-04 - val_loss: 2.6951e-04\n",
            "Epoch 132/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7687e-04 - val_loss: 2.8079e-04\n",
            "Epoch 133/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6683e-04 - val_loss: 2.7035e-04\n",
            "Epoch 134/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6973e-04 - val_loss: 2.6573e-04\n",
            "Epoch 135/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4413e-04 - val_loss: 2.7829e-04\n",
            "Epoch 136/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8060e-04 - val_loss: 2.6551e-04\n",
            "Epoch 137/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7575e-04 - val_loss: 2.7073e-04\n",
            "Epoch 138/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7497e-04 - val_loss: 2.6078e-04\n",
            "Epoch 139/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7614e-04 - val_loss: 2.7493e-04\n",
            "Epoch 140/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6495e-04 - val_loss: 2.7539e-04\n",
            "Epoch 141/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8259e-04 - val_loss: 2.5824e-04\n",
            "Epoch 142/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7181e-04 - val_loss: 2.7806e-04\n",
            "Epoch 143/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7389e-04 - val_loss: 2.5522e-04\n",
            "Epoch 144/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7025e-04 - val_loss: 2.5544e-04\n",
            "Epoch 145/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7276e-04 - val_loss: 3.2642e-04\n",
            "Epoch 146/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8234e-04 - val_loss: 2.6012e-04\n",
            "Epoch 147/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5428e-04 - val_loss: 2.5275e-04\n",
            "Epoch 148/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3760e-04 - val_loss: 2.5302e-04\n",
            "Epoch 149/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4760e-04 - val_loss: 2.5445e-04\n",
            "Epoch 150/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5793e-04 - val_loss: 2.6338e-04\n",
            "Epoch 151/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7242e-04 - val_loss: 2.4796e-04\n",
            "Epoch 152/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4520e-04 - val_loss: 2.4647e-04\n",
            "Epoch 153/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4383e-04 - val_loss: 2.4538e-04\n",
            "Epoch 154/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6483e-04 - val_loss: 2.4915e-04\n",
            "Epoch 155/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5524e-04 - val_loss: 2.4267e-04\n",
            "Epoch 156/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6050e-04 - val_loss: 2.4655e-04\n",
            "Epoch 157/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5706e-04 - val_loss: 2.4515e-04\n",
            "Epoch 158/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7213e-04 - val_loss: 2.3996e-04\n",
            "Epoch 159/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4627e-04 - val_loss: 2.5300e-04\n",
            "Epoch 160/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3444e-04 - val_loss: 2.5132e-04\n",
            "Epoch 161/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5148e-04 - val_loss: 2.3874e-04\n",
            "Epoch 162/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3844e-04 - val_loss: 2.5114e-04\n",
            "Epoch 163/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4010e-04 - val_loss: 2.3987e-04\n",
            "Epoch 164/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4517e-04 - val_loss: 2.5264e-04\n",
            "Epoch 165/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5616e-04 - val_loss: 2.6661e-04\n",
            "Epoch 166/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5314e-04 - val_loss: 2.3632e-04\n",
            "Epoch 167/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3956e-04 - val_loss: 2.3195e-04\n",
            "Epoch 168/500\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.4216e-04 - val_loss: 2.5538e-04\n",
            "Epoch 169/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2496e-04 - val_loss: 2.2919e-04\n",
            "Epoch 170/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6008e-04 - val_loss: 2.4477e-04\n",
            "Epoch 171/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4680e-04 - val_loss: 2.7775e-04\n",
            "Epoch 172/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6988e-04 - val_loss: 2.3705e-04\n",
            "Epoch 173/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4360e-04 - val_loss: 2.5075e-04\n",
            "Epoch 174/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6877e-04 - val_loss: 2.3089e-04\n",
            "Epoch 175/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4130e-04 - val_loss: 2.2516e-04\n",
            "Epoch 176/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2229e-04 - val_loss: 2.2178e-04\n",
            "Epoch 177/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2568e-04 - val_loss: 2.2479e-04\n",
            "Epoch 178/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2127e-04 - val_loss: 2.2049e-04\n",
            "Epoch 179/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2836e-04 - val_loss: 2.6373e-04\n",
            "Epoch 180/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3615e-04 - val_loss: 2.4236e-04\n",
            "Epoch 181/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4341e-04 - val_loss: 2.3222e-04\n",
            "Epoch 182/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4601e-04 - val_loss: 2.3077e-04\n",
            "Epoch 183/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5342e-04 - val_loss: 2.1689e-04\n",
            "Epoch 184/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2275e-04 - val_loss: 2.1682e-04\n",
            "Epoch 185/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3828e-04 - val_loss: 2.4779e-04\n",
            "Epoch 186/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1799e-04 - val_loss: 2.1738e-04\n",
            "Epoch 187/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2552e-04 - val_loss: 2.3309e-04\n",
            "Epoch 188/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2740e-04 - val_loss: 2.2104e-04\n",
            "Epoch 189/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2754e-04 - val_loss: 2.1715e-04\n",
            "Epoch 190/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0046e-04 - val_loss: 2.2183e-04\n",
            "Epoch 191/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2356e-04 - val_loss: 2.2934e-04\n",
            "Epoch 192/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2333e-04 - val_loss: 2.3311e-04\n",
            "Epoch 193/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3428e-04 - val_loss: 2.2080e-04\n",
            "Epoch 194/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0815e-04 - val_loss: 2.2268e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wJGS8hJGqZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a504301a-fd5a-473b-acbf-13944605eb9b"
      },
      "source": [
        "plt.plot(history.epoch,history.history.get('loss'),label='loss')\r\n",
        "plt.plot(history.epoch,history.history.get('val_loss'),label='val_loss')\r\n",
        "plt.legend()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5d971b7470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4pxuEDadJw"
      },
      "source": [
        "target_test_predict = model.predict(target_test_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_o6IpzWCSl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893a06ec-eb69-49ce-903a-f2225c714028"
      },
      "source": [
        "target_test_predict"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5602364 ],\n",
              "       [0.52566487],\n",
              "       [0.75212467],\n",
              "       ...,\n",
              "       [0.08351812],\n",
              "       [0.49134818],\n",
              "       [0.41763687]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmJjMkFobPX4"
      },
      "source": [
        "np.savetxt('test_predicted.txt',(target_test_predict),header='y',comments='')"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}